#!/usr/bin/env bash
set -eu

# Bootstrap script to create a GKE cluster and a cloudSQL database needed to deploy Concourse.
#
# Please login in to gcp before executing this script
#  >> gcloud auth login
#
# Go to the gcp project where the concourse cluster should be located
#  >> gcloud config set project <project id>
#

if [[ ${PROJECT_REGION:-"unset"} == "unset" ]]; then
  echo "Please set environment variable \"PROJECT_REGION\" to the desired region."
  echo "For a list of all regions, see https://cloud.google.com/compute/docs/regions-zones"
  exit 1
fi

PROJECT_ID=$(gcloud config get-value core/project 2>/dev/null)
CONCOURSE_SA=concourse
CNRM_SA=cnrm-system
CLUSTER_NAME=concourse

# create service account for infra resources
gcloud iam service-accounts create ${CONCOURSE_SA}
gcloud projects add-iam-policy-binding ${PROJECT_ID} --member="serviceAccount:${CONCOURSE_SA}@${PROJECT_ID}.iam.gserviceaccount.com" --role="roles/owner"

# create prostgres instance
gcloud secrets create postgres-admin
openssl rand -base64 14 | gcloud secrets versions add postgres-admin --data-file=-

gcloud sql instances create concourse \
  --database-version=POSTGRES_13 \
  --availability-type=regional \
  --region="$PROJECT_REGION" \
  --cpu=1 \
  --memory=4 \
  --enable-point-in-time-recovery \
  --root-password=$(gcloud secrets versions access $(gcloud --format=json secrets versions list postgres-admin | jq -r 'map(select(.state == "ENABLED"))[0].name'))

# create application databases on postgres instance
for ds in concourse credhub uaa; do \
  gcloud sql databases create $ds --instance concourse; \
done

# create gke cluster and dedicated node pool for concourse workers using preemptible vms
gcloud container clusters create $CLUSTER_NAME \
  --release-channel regular \
  #--addons ConfigConnector \ TODO: currently not included in the cluster
  #--workload-pool=${PROJECT_ID}.svc.id.goog \
  --region "$PROJECT_REGION" \
  --logging=SYSTEM,WORKLOAD \
  --monitoring=SYSTEM \
  --num-nodes=1 \
  --min-nodes=1 \
  --max-nodes=3 \
  --enable-autoscaling \
  --cluster-ipv4-cidr=10.104.0.0/14 \
  --master-ipv4-cidr=172.16.0.32/28 \
  --enable-private-nodes \
  --enable-ip-alias \
  --no-enable-master-authorized-networks \
  --machine-type=n1-standard-4 \
  --service-account=${CONCOURSE_SA}@${PROJECT_ID}.iam.gserviceaccount.com

gcloud container node-pools create concourse-workers \
  --cluster=$CLUSTER_NAME \
  --machine-type=n1-standard-4 \
  --enable-autoscaling \
  --enable-autoupgrade \
  --num-nodes=2 \
  --min-nodes=2 \
  --max-nodes=4 \
  --local-ssd-count 1 \
  --region "$PROJECT_REGION" \
  --tags=workers \
  --node-taints=workers=true:NoSchedule \
  --service-account=${CONCOURSE_SA}@${PROJECT_ID}.iam.gserviceaccount.com

# configure outgoing traffic
gcloud compute routers create nat-router \
    --network default \
    --region "$PROJECT_REGION"

gcloud compute routers nats create nat-config \
    --router-region "$PROJECT_REGION" \
    --router nat-router \
    --nat-all-subnet-ip-ranges \
    --auto-allocate-nat-external-ips \
    --min-ports-per-vm=4095 \
    --tcp-established-idle-timeout=60 

# setup firewall to connect from gke to internal instance vms
# https://cloud.google.com/kubernetes-engine/docs/troubleshooting#autofirewall
network=$(gcloud container clusters describe $CLUSTER_NAME --format=get"(network)" --region "$PROJECT_REGION")
ipv4_cidr=$(gcloud container clusters describe $CLUSTER_NAME --format=get"(clusterIpv4Cidr)" --region "$PROJECT_REGION")
gcloud compute firewall-rules create "$CLUSTER_NAME-to-all-vms-on-network" \
  --network="${network}" --source-ranges="${ipv4_cidr}" \
  --allow=tcp,udp,icmp,esp,ah,sctp

# create subnets required for deploying bosh workloads and running  tests
gcloud compute networks subnets create bosh-integration1 --network=default --range=10.100.0.0/24 --region="$PROJECT_REGION"
gcloud compute networks subnets create bosh-integration2 --network=default --range=10.100.1.0/24 --region="$PROJECT_REGION"
gcloud compute networks subnets create bosh-integration3 --network=default --range=10.100.2.0/24 --region="$PROJECT_REGION"

# configure config connector (translation of k8s resources -> gcp resources)
gcloud iam service-accounts create ${CNRM_SA}
# bind the roles/owner to the service account
gcloud projects add-iam-policy-binding ${PROJECT_ID} \
  --member="serviceAccount:${CNRM_SA}@${PROJECT_ID}.iam.gserviceaccount.com" --role="roles/owner"
# bind roles/iam.workloadIdentityUser to the cnrm-controller-manager Kubernetes Service Account
# in the cnrm-system Namespace
gcloud iam service-accounts add-iam-policy-binding ${CNRM_SA}@${PROJECT_ID}.iam.gserviceaccount.com \
  --member="serviceAccount:${PROJECT_ID}.svc.id.goog[${CNRM_SA}/cnrm-controller-manager]" \
  --role="roles/iam.workloadIdentityUser"
